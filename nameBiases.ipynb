{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-18T10:24:39.771461Z",
     "start_time": "2026-02-18T10:24:39.749706Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read semicolon-separated CSV, Norwegian locale for numbers\n",
    "#df = pd.read_csv('12891_Etternavn.csv', sep=';', decimal=',', thousands=' ')\n",
    "df = pd.read_csv('datasetsFromWeb/12891_Etternavn.csv', sep=';', decimal=',', thousands=' ')\n",
    "\n",
    "df['Personer'] = df['Personer'].astype(str).str.replace(' ', '')\n",
    "df['Personer'] = pd.to_numeric(df['Personer'].replace('.', pd.NA), errors='coerce')\n",
    "\n",
    "#\n",
    "top_10 = df.dropna().sort_values('Personer', ascending=False).head(4)\n",
    "\n",
    "print(\"Top 10 most common surnames:\")\n",
    "#print(top_10)\n",
    "\n",
    "#top_10.to_csv('topSurnames.csv', index=False)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most common surnames:\n",
      "             Navn  Personer\n",
      "1134       Hansen   47879.0\n",
      "1534     Johansen   45604.0\n",
      "2388        Olsen   44682.0\n",
      "1805       Larsen   34833.0\n",
      "128      Andersen   34382.0\n",
      "2454     Pedersen   32685.0\n",
      "2263       Nilsen   31657.0\n",
      "1713  Kristiansen   21809.0\n",
      "1520       Jensen   21253.0\n",
      "1582      Karlsen   19636.0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T13:28:56.991281Z",
     "start_time": "2026-02-18T13:28:56.271302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import kagglehub\n",
    "\n",
    "#Download latest version\n",
    "#path = kagglehub.dataset_download(\"erpel1/forenames-and-surnames-with-gender-and-country\")\n",
    "\n",
    "#print(\"Path to dataset files:\", path)"
   ],
   "id": "b71bb314fb5e5d1d",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T13:28:57.635708Z",
     "start_time": "2026-02-18T13:28:57.004839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#path = kagglehub.dataset_download(\"beridzeg45/most-common-surnames-in-the-world\")\n",
    "\n",
    "#print(\"Path to dataset files:\", path)"
   ],
   "id": "575345edd7e1403e",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T13:29:15.672430Z",
     "start_time": "2026-02-18T13:28:57.704468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read semicolon-separated CSV, Norwegian locale for numbers\n",
    "#df = pd.read_csv('12891_Etternavn.csv', sep=';', decimal=',', thousands=' ')\n",
    "df = pd.read_csv('FirstNames/forenames.csv', sep=',', decimal='.', thousands=' ')\n",
    "\n",
    "df['count'] = df['count'].astype(str).str.replace(' ', '')\n",
    "df['count'] = pd.to_numeric(df['count'].replace('.', pd.NA), errors='coerce')\n",
    "\n",
    "#\n",
    "top_10 = df.dropna().sort_values('count', ascending=False).head(4)\n",
    "\n",
    "print(\"Top 10 most common surnames:\")\n",
    "print(top_10)\n"
   ],
   "id": "8b2bc8cc7f3051ab",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanur\\AppData\\Local\\Temp\\ipykernel_29412\\3583536464.py:5: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('FirstNames/forenames.csv', sep=',', decimal='.', thousands=' ')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most common surnames:\n",
      "        forename gender country    count\n",
      "2513920     Ù…Ø­Ù…Ø¯      M      EG  1584172\n",
      "2513921    Ahmed      M      EG  1487556\n",
      "2513922  Mohamed      M      EG  1446833\n",
      "2513923     Ø§Ø­Ù…Ø¯      M      EG   936642\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T13:21:35.337276Z",
     "start_time": "2026-02-18T13:21:18.377590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "# Method 1: Using csv module\n",
    "def count_names_csv_method(filename):\n",
    "    names_to_count = {'mohammed', 'mohamed'}\n",
    "    counts = defaultdict(int)\n",
    "\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            forename = row['forename'].lower().strip()\n",
    "            if forename in names_to_count:\n",
    "                count = int(row['count'])\n",
    "                counts[forename] += count\n",
    "\n",
    "    return counts\n",
    "\n",
    "# Method 2: Using pandas (if you have it installed)\n",
    "def count_names_pandas(filename):\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        df = pd.read_csv(filename)\n",
    "        names_to_count = ['mohammed', 'mohamed']\n",
    "\n",
    "        # Filter and sum counts\n",
    "        mask = df['forename'].str.lower().isin(names_to_count)\n",
    "        result = df[mask].groupby(df['forename'].str.lower())['count'].sum()\n",
    "\n",
    "        return result.to_dict()\n",
    "    except ImportError:\n",
    "        print(\"Pandas not installed, using csv method instead\")\n",
    "        return count_names_csv_method(filename)\n",
    "\n",
    "# Method 3: Simple manual parsing (no csv module needed)\n",
    "def count_names_simple(filename):\n",
    "    names_to_count = {'mohammed', 'mohamed'}\n",
    "    counts = defaultdict(int)\n",
    "\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        # Skip header\n",
    "        next(file)\n",
    "\n",
    "        for line in file:\n",
    "            parts = line.strip().split(',')\n",
    "            if len(parts) >= 4:  # Make sure we have enough columns\n",
    "                forename = parts[0].lower().strip()\n",
    "                if forename in names_to_count:\n",
    "                    count = int(parts[3])  # count is the 4th column\n",
    "                    counts[forename] += count\n",
    "\n",
    "    return counts\n",
    "\n",
    "# Run the code\n",
    "filename = 'FirstNames/forenames.csv'  # Replace with your actual filename\n",
    "\n",
    "# Choose one method\n",
    "result = count_names_csv_method(filename)\n",
    "# result = count_names_pandas(filename)  # Alternative\n",
    "# result = count_names_simple(filename)  # Alternative\n",
    "\n",
    "# Print results\n",
    "print(\"Total counts:\")\n",
    "for name, count in result.items():\n",
    "    print(f\"{name}: {count}\")\n",
    "\n",
    "print(f\"\\nTotal combined: {sum(result.values())}\")"
   ],
   "id": "62d638d868b2bd81",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total counts:\n",
      "mohammed: 924171\n",
      "mohamed: 2835189\n",
      "\n",
      "Total combined: 3759360\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T13:46:23.964731Z",
     "start_time": "2026-02-18T13:46:09.561562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "\n",
    "def analyze_names_by_country(filename, country_code='NO'):\n",
    "    \"\"\"\n",
    "    Analyze name popularity for a specific country code\n",
    "    \"\"\"\n",
    "    # Separate by gender\n",
    "    male_names = []\n",
    "    female_names = []\n",
    "    all_names = []\n",
    "\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            if row['country'] == country_code:\n",
    "                name = row['forename']\n",
    "                gender = row['gender']\n",
    "                count = int(row['count'])\n",
    "\n",
    "                all_names.append((name, gender, count))\n",
    "\n",
    "                if gender == 'M':\n",
    "                    male_names.append((name, count))\n",
    "                elif gender == 'F':\n",
    "                    female_names.append((name, count))\n",
    "\n",
    "    # Sort by count (most popular first)\n",
    "    male_names.sort(key=lambda x: x[1], reverse=True)\n",
    "    female_names.sort(key=lambda x: x[1], reverse=True)\n",
    "    all_names.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    return {\n",
    "        'all': all_names,\n",
    "        'male': male_names,\n",
    "        'female': female_names,\n",
    "        'total_names': len(all_names),\n",
    "        'total_males': len(male_names),\n",
    "        'total_females': len(female_names),\n",
    "        'country': country_code\n",
    "    }\n",
    "\n",
    "def print_top_names(results, top_n=10):\n",
    "    \"\"\"\n",
    "    Print top N names for each category\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"TOP NAMES FOR COUNTRY: {results['country']}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    print(f\"\\nðŸ“Š OVERALL TOP {top_n} NAMES:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, (name, gender, count) in enumerate(results['all'][:top_n], 1):\n",
    "        gender_symbol = 'M' if gender == 'M' else 'W'\n",
    "        print(f\"{i:2}. {gender_symbol} {name:<15} {count:>8,}\")\n",
    "\n",
    "    print(f\"\\nM MALE TOP {top_n} NAMES:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, (name, count) in enumerate(results['male'][:top_n], 1):\n",
    "        print(f\"{i:2}. {name:<15} {count:>8,}\")\n",
    "\n",
    "    print(f\"\\nW FEMALE TOP {top_n} NAMES:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, (name, count) in enumerate(results['female'][:top_n], 1):\n",
    "        print(f\"{i:2}. {name:<15} {count:>8,}\")\n",
    "\n",
    "    print(f\"\\nðŸ“ˆ Summary for {results['country']}:\")\n",
    "    print(f\"   Total unique names: {results['total_names']:,}\")\n",
    "    print(f\"   Male names: {results['total_males']:,}\")\n",
    "    print(f\"   Female names: {results['total_females']:,}\")\n",
    "\n",
    "# Alternative: Using pandas (more concise)\n",
    "def analyze_with_pandas(filename, country_code='NO'):\n",
    "    \"\"\"\n",
    "    Using pandas for more advanced analysis\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import pandas as pd\n",
    "\n",
    "        # Read CSV\n",
    "        df = pd.read_csv(filename)\n",
    "\n",
    "        # Filter for Norway\n",
    "        norway_df = df[df['country'] == country_code].copy()\n",
    "\n",
    "        if norway_df.empty:\n",
    "            print(f\"No data found for country code: {country_code}\")\n",
    "            return\n",
    "\n",
    "        norway_df = norway_df.sort_values('count', ascending=False)\n",
    "\n",
    "        # Top overall\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"TOP NAMES FOR COUNTRY: {country_code}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        print(f\"\\nðŸ“Š TOP 10 OVERALL:\")\n",
    "        print(\"-\" * 40)\n",
    "        for idx, row in norway_df.head(10).iterrows():\n",
    "            gender_symbol = 'M' if row['gender'] == 'M' else 'W'\n",
    "            print(f\"{gender_symbol} {row['forename']:<15} {row['count']:>8,}\")\n",
    "\n",
    "        # Top by gender\n",
    "        print(f\"\\nM TOP 10 MALE:\")\n",
    "        print(\"-\" * 40)\n",
    "        male_df = norway_df[norway_df['gender'] == 'M'].head(10)\n",
    "        for idx, row in male_df.iterrows():\n",
    "            print(f\"{row['forename']:<15} {row['count']:>8,}\")\n",
    "\n",
    "        print(f\"\\nW TOP 10 FEMALE:\")\n",
    "        print(\"-\" * 40)\n",
    "        female_df = norway_df[norway_df['gender'] == 'F'].head(10)\n",
    "        for idx, row in female_df.iterrows():\n",
    "            print(f\"{row['forename']:<15} {row['count']:>8,}\")\n",
    "\n",
    "        # Additional statistics\n",
    "        print(f\"\\nðŸ“ˆ Statistics for {country_code}:\")\n",
    "        print(f\"   Total unique names: {len(norway_df):,}\")\n",
    "        print(f\"   Male names: {len(male_df):,}\")\n",
    "        print(f\"   Female names: {len(female_df):,}\")\n",
    "        print(f\"   Most common name: {norway_df.iloc[0]['forename']} ({norway_df.iloc[0]['count']:,})\")\n",
    "\n",
    "        return norway_df\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"Pandas not installed. Install with: pip install pandas\")\n",
    "        return None\n",
    "\n",
    "# Run the analysis\n",
    "filename = 'FirstNames/forenames.csv'  # Replace with your actual filename\n",
    "\n",
    "# Method 1: Using the custom function\n",
    "print(\"Method 1: Custom CSV parsing\")\n",
    "results = analyze_names_by_country(filename, 'AE')\n",
    "print_top_names(results, top_n=10)\n",
    "\n",
    "def save_results_to_file(results, filename='norway_names.txt'):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"NAME ANALYSIS FOR COUNTRY: {results['country']}\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\\n\")\n",
    "\n",
    "        f.write(\"ALL NAMES (sorted by popularity):\\n\")\n",
    "        f.write(\"-\"*40 + \"\\n\")\n",
    "        for name, gender, count in results['all']:\n",
    "            f.write(f\"{name:<15} ({gender}) {count:>8,}\\n\")\n",
    "\n",
    "    print(f\"\\nðŸ’¾ Results saved to {filename}\")\n",
    "\n",
    "# Uncomment to save results\n",
    "# save_results_to_file(results)"
   ],
   "id": "10383c13d578fed9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1: Custom CSV parsing\n",
      "\n",
      "==================================================\n",
      "TOP NAMES FOR COUNTRY: AE\n",
      "==================================================\n",
      "\n",
      "ðŸ“Š OVERALL TOP 10 NAMES:\n",
      "----------------------------------------\n",
      " 1. M Md               132,181\n",
      " 2. M Muhammad          84,884\n",
      " 3. M Mohammed          51,575\n",
      " 4. M Abdul             48,408\n",
      " 5. M Ali               40,866\n",
      " 6. M Mohammad          39,532\n",
      " 7. M Ahmed             31,549\n",
      " 8. M Mohamed           30,575\n",
      " 9. M Ù…Ø­Ù…Ø¯              25,879\n",
      "10. M Ahmad             15,871\n",
      "\n",
      "M MALE TOP 10 NAMES:\n",
      "----------------------------------------\n",
      " 1. Md               132,181\n",
      " 2. Muhammad          84,884\n",
      " 3. Mohammed          51,575\n",
      " 4. Abdul             48,408\n",
      " 5. Ali               40,866\n",
      " 6. Mohammad          39,532\n",
      " 7. Ahmed             31,549\n",
      " 8. Mohamed           30,575\n",
      " 9. Ù…Ø­Ù…Ø¯              25,879\n",
      "10. Ahmad             15,871\n",
      "\n",
      "W FEMALE TOP 10 NAMES:\n",
      "----------------------------------------\n",
      " 1. Md                 8,381\n",
      " 2. Sara               6,774\n",
      " 3. Fatima             5,862\n",
      " 4. Maryam             3,846\n",
      " 5. Mariam             3,468\n",
      " 6. Aisha              3,353\n",
      " 7. Sarah              2,834\n",
      " 8. Noor               2,743\n",
      " 9. Reem               2,660\n",
      "10. Sana               2,393\n",
      "\n",
      "ðŸ“ˆ Summary for AE:\n",
      "   Total unique names: 280,975\n",
      "   Male names: 192,620\n",
      "   Female names: 74,875\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T08:47:14.200558Z",
     "start_time": "2026-02-25T08:43:28.765444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "def top_names_by_country(input_file, output_file, countries, encoding='utf-8'):\n",
    "    \"\"\"\n",
    "    Extract top 10 names per selected country from a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        input_file (str): Path to input CSV (no header, columns: name, gender, country, count)\n",
    "        output_file (str): Path to output CSV (will contain same columns with top 10 per country)\n",
    "        countries (list): List of country codes to process (e.g., ['AE', 'US'])\n",
    "        encoding (str): File encoding (default 'utf-8')\n",
    "    \"\"\"\n",
    "    columns = ['name', 'gender', 'country', 'count']\n",
    "    df = pd.read_csv(input_file, names=columns, encoding=encoding)\n",
    "\n",
    "    df['count'] = pd.to_numeric(df['count'], errors='coerce')\n",
    "    df_filtered = df[df['country'].isin(countries)]\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        print(\"No data found for the specified countries.\")\n",
    "        return\n",
    "\n",
    "    top_dfs = []\n",
    "    for country in countries:\n",
    "        country_df = df_filtered[df_filtered['country'] == country]\n",
    "        top10 = country_df.sort_values('count', ascending=False).head(10)\n",
    "        top_dfs.append(top10)\n",
    "\n",
    "\n",
    "    result = pd.concat(top_dfs, ignore_index=True)\n",
    "\n",
    "    result.to_csv(output_file, index=False, encoding=encoding)\n",
    "    print(f\"Top 10 names per country saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    INPUT_FILE = \"surenames/surnames.csv\"\n",
    "    OUTPUT_FILE = \"top10PerCountry/top10SurenamesPerCountry.csv\"\n",
    "    #COUNTRIES = ['AE', 'SA', 'US']\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    uniqueCountries = df['country'].unique()\n",
    "    print(uniqueCountries)\n",
    "\n",
    "    top_names_by_country(INPUT_FILE, OUTPUT_FILE, uniqueCountries)"
   ],
   "id": "a6506b3cd0b95b9a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanur\\AppData\\Local\\Temp\\ipykernel_16112\\1777525783.py:39: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(INPUT_FILE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AE' 'AF' 'AL' 'AO' 'AR' 'AT' 'AZ' 'BD' 'BE' 'BF' 'BG' 'BH' 'BI' 'BN'\n",
      " 'BO' 'BR' 'BW' 'CA' 'CH' 'CL' 'CM' 'CN' 'CO' 'CR' 'CY' 'CZ' 'DE' 'DJ'\n",
      " 'DK' 'DZ' 'EC' 'EE' 'EG' 'ES' 'ET' 'FI' 'FJ' 'FR' 'GB' 'GE' 'GH' 'GR'\n",
      " 'GT' 'HK' 'HN' 'HR' 'HT' 'HU' 'ID' 'IE' 'IL' 'IN' 'IQ' 'IR' 'IS' 'IT'\n",
      " 'JM' 'JO' 'JP' 'KH' 'KR' 'KW' 'KZ' 'LB' 'LT' 'LU' 'LY' 'MA' 'MD' 'MO'\n",
      " 'MT' 'MU' 'MV' 'MX' 'MY' nan 'NG' 'NL' 'NO' 'OM' 'PA' 'PE' 'PH' 'PL' 'PR'\n",
      " 'PS' 'PT' 'QA' 'RS' 'RU' 'SA' 'SD' 'SE' 'SG' 'SI' 'SV' 'SY' 'TM' 'TN'\n",
      " 'TR' 'TW' 'US' 'UY' 'YE' 'ZA']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanur\\AppData\\Local\\Temp\\ipykernel_16112\\1777525783.py:14: DtypeWarning: Columns (1,3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file, names=columns, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 names per country saved to top10PerCountry/topSurenamesPerCountry.csv\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T09:01:01.565675Z",
     "start_time": "2026-02-25T09:01:01.431161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "def combine_names_and_surnames(\n",
    "    first_names_file,\n",
    "    surnames_file,\n",
    "    output_file,\n",
    "    top_n_first=None,\n",
    "    top_m_surnames=None,\n",
    "    sample_size=None,\n",
    "    encoding='utf-8'\n",
    "):\n",
    "    \"\"\"\n",
    "    Combine first names and surnames by country and gender.\n",
    "\n",
    "    Parameters:\n",
    "        first_names_file (str): path to CSV with first names (name,gender,country,count)\n",
    "        surnames_file (str): path to CSV with surnames (surname,gender,country,count)\n",
    "        output_file (str): where to save the combined results\n",
    "        top_n_first (int, optional): keep only top N first names per (country,gender) by count\n",
    "        top_m_surnames (int, optional): keep only top M surnames per (country,gender) by count\n",
    "        sample_size (int, optional): randomly sample this many combinations per (country,gender)\n",
    "        encoding (str): file encoding (default utf-8)\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    first_df = pd.read_csv(first_names_file, encoding=encoding)\n",
    "    surnames_df = pd.read_csv(surnames_file, encoding=encoding)\n",
    "\n",
    "    # Ensure count is numeric\n",
    "    first_df['count'] = pd.to_numeric(first_df['count'], errors='coerce')\n",
    "    surnames_df['count'] = pd.to_numeric(surnames_df['count'], errors='coerce')\n",
    "\n",
    "    # Drop rows with missing gender/country\n",
    "    first_df = first_df.dropna(subset=['gender', 'country'])\n",
    "    surnames_df = surnames_df.dropna(subset=['gender', 'country'])\n",
    "\n",
    "    # Keep only top N first names per (country,gender)\n",
    "    if top_n_first:\n",
    "        first_df = (first_df\n",
    "                    .sort_values(['country', 'gender', 'count'], ascending=[True, True, False])\n",
    "                    .groupby(['country', 'gender'])\n",
    "                    .head(top_n_first)\n",
    "                    .reset_index(drop=True))\n",
    "\n",
    "    if top_m_surnames:\n",
    "        surnames_df = (surnames_df\n",
    "                       .sort_values(['country', 'gender', 'count'], ascending=[True, True, False])\n",
    "                       .groupby(['country', 'gender'])\n",
    "                       .head(top_m_surnames)\n",
    "                       .reset_index(drop=True))\n",
    "\n",
    "    first_df = first_df.rename(columns={'name': 'first_name', 'count': 'first_count'})\n",
    "    surnames_df = surnames_df.rename(columns={'name': 'last_name', 'count': 'last_count'})\n",
    "\n",
    "    combined = pd.merge(first_df, surnames_df, on=['country', 'gender'], how='inner')\n",
    "\n",
    "    combined['combined_count'] = combined['first_count'] * combined['last_count']\n",
    "\n",
    "    result = combined[['first_name', 'last_name', 'gender', 'country', 'combined_count']]\n",
    "\n",
    "    # Optional: sample a fixed number of rows per group\n",
    "    #Other solution is if surename gender = null\n",
    "    #I think that this means that the surenamesname applies to both genders\n",
    "    #\n",
    "    if sample_size:\n",
    "        result = (result\n",
    "                  .groupby(['country', 'gender'])\n",
    "                  .apply(lambda x: x.sample(n=min(sample_size, len(x)), random_state=42),\n",
    "                         include_groups=False)   #\n",
    "                  .reset_index())\n",
    "\n",
    "    result.to_csv(output_file, index=False, encoding=encoding)\n",
    "    print(f\"Combined data saved to {output_file} with {len(result)} rows.\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    combine_names_and_surnames(\n",
    "        first_names_file=\"top10PerCountry/top10ForenamesPerCountry.csv\",\n",
    "        surnames_file=\"top10PerCountry/top10SurenamesPerCountry.csv\",\n",
    "        output_file=\"full_names.csv\",\n",
    "        top_n_first=50,\n",
    "        top_m_surnames=50,\n",
    "        sample_size=1000\n",
    "    )"
   ],
   "id": "3240e30aaf3615a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data saved to full_names.csv with 5873 rows.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "def combine_names_and_surnames(\n",
    "    first_names_file,\n",
    "    surnames_file,\n",
    "    output_file,\n",
    "    top_n_first=None,\n",
    "    top_m_surnames=None,\n",
    "    sample_size=None,\n",
    "    encoding='utf-8'\n",
    "):\n",
    "    \"\"\"\n",
    "    Combine first names and surnames by country and gender.\n",
    "    Handles surnames with null gender by treating them as genderâ€‘neutral\n",
    "    and expanding them to both 'M' and 'F'.\n",
    "    \"\"\"\n",
    "    # --- Load data ---\n",
    "    first_df = pd.read_csv(first_names_file, encoding=encoding)\n",
    "    surnames_df = pd.read_csv(surnames_file, encoding=encoding)\n",
    "\n",
    "    # Ensure count is numeric\n",
    "    first_df['count'] = pd.to_numeric(first_df['count'], errors='coerce')\n",
    "    surnames_df['count'] = pd.to_numeric(surnames_df['count'], errors='coerce')\n",
    "\n",
    "    # Drop rows with missing country (essential for grouping)\n",
    "    first_df = first_df.dropna(subset=['country'])\n",
    "    surnames_df = surnames_df.dropna(subset=['country'])\n",
    "\n",
    "    # For first names, drop rows with missing gender (assume they are all specified)\n",
    "    first_df = first_df.dropna(subset=['gender'])\n",
    "\n",
    "    # --- Handle genderâ€‘neutral surnames ---\n",
    "    # Split surnames into those with gender and those without\n",
    "    surnames_with_gender = surnames_df[surnames_df['gender'].notna()].copy()\n",
    "    surnames_neutral = surnames_df[surnames_df['gender'].isna()].copy()\n",
    "\n",
    "    # Expand neutral surnames to both genders (preserving the original count)\n",
    "    if not surnames_neutral.empty:\n",
    "        neutral_m = surnames_neutral.assign(gender='M')\n",
    "        neutral_f = surnames_neutral.assign(gender='F')\n",
    "        surnames_df = pd.concat([surnames_with_gender, neutral_m, neutral_f], ignore_index=True)\n",
    "    else:\n",
    "        surnames_df = surnames_with_gender\n",
    "\n",
    "    # Now surnames_df has no null gender; any remaining nulls (if any) can be dropped\n",
    "    surnames_df = surnames_df.dropna(subset=['gender'])\n",
    "\n",
    "    # --- Keep only top N first names per (country, gender) ---\n",
    "    if top_n_first:\n",
    "        first_df = (first_df\n",
    "                    .sort_values(['country', 'gender', 'count'], ascending=[True, True, False])\n",
    "                    .groupby(['country', 'gender'])\n",
    "                    .head(top_n_first)\n",
    "                    .reset_index(drop=True))\n",
    "\n",
    "    # --- Keep only top M surnames per (country, gender) ---\n",
    "    if top_m_surnames:\n",
    "        surnames_df = (surnames_df\n",
    "                       .sort_values(['country', 'gender', 'count'], ascending=[True, True, False])\n",
    "                       .groupby(['country', 'gender'])\n",
    "                       .head(top_m_surnames)\n",
    "                       .reset_index(drop=True))\n",
    "\n",
    "    # --- Prepare for merge ---\n",
    "    first_df = first_df.rename(columns={'name': 'first_name', 'count': 'first_count'})\n",
    "    surnames_df = surnames_df.rename(columns={'name': 'last_name', 'count': 'last_count'})\n",
    "#\n",
    "    combined = pd.merge(first_df, surnames_df, on=['country', 'gender'], how='inner')\n",
    "    combined['combined_count'] = combined['first_count'] * combined['last_count']\n",
    "    result = combined[['first_name', 'last_name', 'gender', 'country', 'combined_count']]\n",
    "\n",
    "    # --- Optional: sample a fixed number of rows per (country, gender) ---\n",
    "    if sample_size:\n",
    "        result = (result\n",
    "                  .groupby(['country', 'gender'])\n",
    "                  .apply(lambda x: x.sample(n=min(sample_size, len(x)), random_state=42),\n",
    "                         include_groups=False)\n",
    "                  .reset_index())  # keeps country and gender as columns\n",
    "\n",
    "    # Save to CSV\n",
    "    result.to_csv(output_file, index=False, encoding=encoding)\n",
    "    print(f\"Combined data saved to {output_file} with {len(result)} rows.\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    combine_names_and_surnames(\n",
    "        first_names_file=\"top10PerCountry/top10ForenamesPerCountry.csv\",\n",
    "        surnames_file=\"top10PerCountry/top10SurenamesPerCountry.csv\",\n",
    "        output_file=\"full_names.csv\",\n",
    "        top_n_first=50,\n",
    "        top_m_surnames=50,\n",
    "        sample_size=1000\n",
    "    )"
   ],
   "id": "a38aa7d79833f8eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "def combine_names_and_surnames(\n",
    "    first_names_file,\n",
    "    surnames_file,\n",
    "    output_file,\n",
    "    top_n_first=None,\n",
    "    top_m_surnames=None,\n",
    "    sample_size=None,\n",
    "    encoding='utf-8'\n",
    "):\n",
    "    \"\"\"\n",
    "    Combine first names and surnames by country and gender.\n",
    "    Handles surnames with null gender by treating them as genderâ€‘neutral\n",
    "    and expanding them to both 'M' and 'F'.\n",
    "    \"\"\"\n",
    "    # --- Load data ---\n",
    "    first_df = pd.read_csv(first_names_file, encoding=encoding)\n",
    "    surnames_df = pd.read_csv(surnames_file, encoding=encoding)\n",
    "\n",
    "    # Ensure count is numeric\n",
    "    first_df['count'] = pd.to_numeric(first_df['count'], errors='coerce')\n",
    "    surnames_df['count'] = pd.to_numeric(surnames_df['count'], errors='coerce')\n",
    "\n",
    "    # Drop rows with missing country (essential for grouping)\n",
    "    first_df = first_df.dropna(subset=['country'])\n",
    "    surnames_df = surnames_df.dropna(subset=['country'])\n",
    "\n",
    "    # For first names, drop rows with missing gender (assume they are all specified)\n",
    "    first_df = first_df.dropna(subset=['gender'])\n",
    "\n",
    "    # --- Handle genderâ€‘neutral surnames ---\n",
    "    # Split surnames into those with gender and those without\n",
    "    surnames_with_gender = surnames_df[surnames_df['gender'].notna()].copy()\n",
    "    surnames_neutral = surnames_df[surnames_df['gender'].isna()].copy()\n",
    "\n",
    "    # Expand neutral surnames to both genders (preserving the original count)\n",
    "    if not surnames_neutral.empty:\n",
    "        neutral_m = surnames_neutral.assign(gender='M')\n",
    "        neutral_f = surnames_neutral.assign(gender='F')\n",
    "        surnames_df = pd.concat([surnames_with_gender, neutral_m, neutral_f], ignore_index=True)\n",
    "    else:\n",
    "        surnames_df = surnames_with_gender\n",
    "\n",
    "    # Now surnames_df has no null gender; any remaining nulls (if any) can be dropped\n",
    "    surnames_df = surnames_df.dropna(subset=['gender'])\n",
    "\n",
    "    # --- Keep only top N first names per (country, gender) ---\n",
    "    if top_n_first:\n",
    "        first_df = (first_df\n",
    "                    .sort_values(['country', 'gender', 'count'], ascending=[True, True, False])\n",
    "                    .groupby(['country', 'gender'])\n",
    "                    .head(top_n_first)\n",
    "                    .reset_index(drop=True))\n",
    "\n",
    "    # --- Keep only top M surnames per (country, gender) ---\n",
    "    if top_m_surnames:\n",
    "        surnames_df = (surnames_df\n",
    "                       .sort_values(['country', 'gender', 'count'], ascending=[True, True, False])\n",
    "                       .groupby(['country', 'gender'])\n",
    "                       .head(top_m_surnames)\n",
    "                       .reset_index(drop=True))\n",
    "\n",
    "    # --- Prepare for merge ---\n",
    "    first_df = first_df.rename(columns={'name': 'first_name', 'count': 'first_count'})\n",
    "    surnames_df = surnames_df.rename(columns={'name': 'last_name', 'count': 'last_count'})\n",
    "\n",
    "    # Inner join on country and gender â€“ only groups present in both files will appear\n",
    "    combined = pd.merge(first_df, surnames_df, on=['country', 'gender'], how='inner')\n",
    "\n",
    "    # Compute combined count (product) as a popularity indicator\n",
    "    combined['combined_count'] = combined['first_count'] * combined['last_count']\n",
    "\n",
    "    # Keep relevant columns\n",
    "    result = combined[['first_name', 'last_name', 'gender', 'country', 'combined_count']]\n",
    "\n",
    "    # --- Optional: sample a fixed number of rows per (country, gender) ---\n",
    "    if sample_size:\n",
    "        result = (result\n",
    "                  .groupby(['country', 'gender'])\n",
    "                  .apply(lambda x: x.sample(n=min(sample_size, len(x)), random_state=42),\n",
    "                         include_groups=False)\n",
    "                  .reset_index())  # keeps country and gender as columns\n",
    "\n",
    "    # Save to CSV\n",
    "    result.to_csv(output_file, index=False, encoding=encoding)\n",
    "    print(f\"Combined data saved to {output_file} with {len(result)} rows.\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    combine_names_and_surnames(\n",
    "        first_names_file=\"top10PerCountry/top10ForenamesPerCountry.csv\",\n",
    "        surnames_file=\"top10PerCountry/top10SurenamesPerCountry.csv\",\n",
    "        output_file=\"full_names.csv\",\n",
    "        top_n_first=50,\n",
    "        top_m_surnames=50,\n",
    "        sample_size=1000\n",
    "    )"
   ],
   "id": "ffac1be7452e425f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T09:47:20.999992Z",
     "start_time": "2026-02-25T09:47:20.530117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "apiKey = pd.read_csv('myAPIKey.csv', sep=';', decimal=',', thousands=' ')['key'][0]\n",
    "#print(apiKey)\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=apiKey,\n",
    ")\n",
    "#api_key=\"<YOUR_OPENROUTER_API_KEY>\",\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"anthropic/claude-3-haiku\",\n",
    "  messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n",
    ")\n",
    "print(completion.choices[0].message.content)\n"
   ],
   "id": "63842dd1b48bc8f9",
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'User not found.', 'code': 401}}",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAuthenticationError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[19]\u001B[39m\u001B[32m, line 12\u001B[39m\n\u001B[32m      6\u001B[39m client = OpenAI(\n\u001B[32m      7\u001B[39m   base_url=\u001B[33m\"\u001B[39m\u001B[33mhttps://openrouter.ai/api/v1\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      8\u001B[39m   api_key=apiKey,\n\u001B[32m      9\u001B[39m )\n\u001B[32m     10\u001B[39m \u001B[38;5;66;03m#api_key=\"<YOUR_OPENROUTER_API_KEY>\",\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m completion = \u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mchat\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcompletions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m  \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43manthropic/claude-3-haiku\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m  \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrole\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43muser\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcontent\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mHello\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m]\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[38;5;28mprint\u001B[39m(completion.choices[\u001B[32m0\u001B[39m].message.content)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\openai\\_utils\\_utils.py:286\u001B[39m, in \u001B[36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    284\u001B[39m             msg = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[32m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    285\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[32m--> \u001B[39m\u001B[32m286\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1204\u001B[39m, in \u001B[36mCompletions.create\u001B[39m\u001B[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001B[39m\n\u001B[32m   1157\u001B[39m \u001B[38;5;129m@required_args\u001B[39m([\u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m], [\u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mstream\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m   1158\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcreate\u001B[39m(\n\u001B[32m   1159\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1201\u001B[39m     timeout: \u001B[38;5;28mfloat\u001B[39m | httpx.Timeout | \u001B[38;5;28;01mNone\u001B[39;00m | NotGiven = not_given,\n\u001B[32m   1202\u001B[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001B[32m   1203\u001B[39m     validate_response_format(response_format)\n\u001B[32m-> \u001B[39m\u001B[32m1204\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1205\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m/chat/completions\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1206\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1207\u001B[39m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[32m   1208\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmessages\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1209\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmodel\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1210\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43maudio\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43maudio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1211\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfrequency_penalty\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1212\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfunction_call\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunction_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1213\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfunctions\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunctions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1214\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mlogit_bias\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1215\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mlogprobs\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1216\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmax_completion_tokens\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_completion_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1217\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmax_tokens\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1218\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmetadata\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1219\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmodalities\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodalities\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1220\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mn\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1221\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mparallel_tool_calls\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mparallel_tool_calls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1222\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mprediction\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprediction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1223\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpresence_penalty\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1224\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mprompt_cache_key\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt_cache_key\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1225\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mprompt_cache_retention\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt_cache_retention\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1226\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mreasoning_effort\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mreasoning_effort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1227\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mresponse_format\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1228\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43msafety_identifier\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43msafety_identifier\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1229\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mseed\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1230\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mservice_tier\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mservice_tier\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1231\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstop\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1232\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstore\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1233\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstream\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1234\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstream_options\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1235\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtemperature\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1236\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtool_choice\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1237\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtools\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1238\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtop_logprobs\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_logprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1239\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtop_p\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1240\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43muser\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1241\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mverbosity\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbosity\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1242\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mweb_search_options\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mweb_search_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1243\u001B[39m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1244\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43mCompletionCreateParamsStreaming\u001B[49m\n\u001B[32m   1245\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\n\u001B[32m   1246\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43mCompletionCreateParamsNonStreaming\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1247\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1248\u001B[39m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1249\u001B[39m \u001B[43m            \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\n\u001B[32m   1250\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1251\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m=\u001B[49m\u001B[43mChatCompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1252\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1253\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mChatCompletionChunk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1254\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\openai\\_base_client.py:1297\u001B[39m, in \u001B[36mSyncAPIClient.post\u001B[39m\u001B[34m(self, path, cast_to, body, content, options, files, stream, stream_cls)\u001B[39m\n\u001B[32m   1288\u001B[39m     warnings.warn(\n\u001B[32m   1289\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mPassing raw bytes as `body` is deprecated and will be removed in a future version. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1290\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mPlease pass raw bytes via the `content` parameter instead.\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1291\u001B[39m         \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m,\n\u001B[32m   1292\u001B[39m         stacklevel=\u001B[32m2\u001B[39m,\n\u001B[32m   1293\u001B[39m     )\n\u001B[32m   1294\u001B[39m opts = FinalRequestOptions.construct(\n\u001B[32m   1295\u001B[39m     method=\u001B[33m\"\u001B[39m\u001B[33mpost\u001B[39m\u001B[33m\"\u001B[39m, url=path, json_data=body, content=content, files=to_httpx_files(files), **options\n\u001B[32m   1296\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m1297\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\openai\\_base_client.py:1070\u001B[39m, in \u001B[36mSyncAPIClient.request\u001B[39m\u001B[34m(self, cast_to, options, stream, stream_cls)\u001B[39m\n\u001B[32m   1067\u001B[39m             err.response.read()\n\u001B[32m   1069\u001B[39m         log.debug(\u001B[33m\"\u001B[39m\u001B[33mRe-raising status error\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1070\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._make_status_error_from_response(err.response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1072\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m   1074\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[33m\"\u001B[39m\u001B[33mcould not resolve response (should never happen)\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[31mAuthenticationError\u001B[39m: Error code: 401 - {'error': {'message': 'User not found.', 'code': 401}}"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d1ea9e91aafd7f12"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
